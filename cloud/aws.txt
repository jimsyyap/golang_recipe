# 6




# 5
A company is hosting a web app on aws using a single amazon ec2 instance that stores user-uploaded documents in an amazon ebs volume. For better scalability and availability, the company duplicated the architecture and created a second ec2 instance and ebs volume in another availability zone, placing both behind an application load balancer. 

After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time. 

What should a solutions architect do to ensure that users can see all of their documents regardless of which ec2 instance they are routed to?

Answer:
The problem:
The issue stems from the fact that each EC2 instance has its own dedicated EBS volume. When a user uploads a document, it's stored on the EBS volume of the EC2 instance they are currently connected to. Subsequent requests may be routed to a different EC2 instance, which doesnt have access to that specific EBS volume.

Solutions:
To ensure users can consistently access all their documents, the architect needs to implement a shared storage solution. Here are the more common approaches:

1. Amazon elastic file system (EFS)
. How it works - EFS provides a fully managed, scalable file system that can be mounted to multiple  EC2 instances concurrently. All instances can read and write to the same EFS file system, making it perfect for shared document storage.
. Benefits - highly scalable, highly available, supports both linux and windows instances.
. Considerations - potentially higher cost than other options, especially for very large amounths of data.

2. Amazon S3
. How it works - s3 is an object storage service. Instead of mounting a file system, applications can directly read and write documents to s3 using the s3 api. Since s3 is a global service, all ec2 instances can access the same s3 bucket.
. Benefits - extremely scalable, highly durable, cost-effective for storing large amonuths of data.
. Considerations - requires changes to the app code to interact with s3 instead of a local file system.

3. Amazon RDS
. How it works - if the app already uses a database, storing document metadata (file name, path, etc) in the database and the actual document content in s3 is another good option.
. Benefits - allows fo rstructured data management and querying of document metadata.
. Considerations - requires changes to the application to integrate with RDS and potentially additional costs depending on the chosen RDS engine.

Implementation steps (example using EFS):
1. create EFS file system - set up an efs file system in the same vpc as your ec2 instances.
2. Mount EFS - mount the EFS file system to both EC2 instances at a common directory path.
3. update application logic - modify the application to read and write documents to the shared EFS mount point instead of the local EBS volume. For exsiting documents, migrate them from the individual EBS volumes to the efs file system.
4. Test - thoroughly test the application to ensure that all documents are accessible regardless of which EC2 instance the user is routed to.

Important considerations:
. Security - ensure appropriate IAM permissions are in place to control access to the shared storage solution.
. Performance - consider the performance requirements of the application when choosing a shared storage solution.
. Data migration - plan and execute a data migration strategy to move existing documents to the shared storage solution.


# 4
An application runs on an amazon ec2 instance in a vpc. The application processes logs that are stored in an amazon s3 bucket. The ec2 instance needs to access the s3 bucket without connectivity to the internet. Which aws solution will provide private network connectivity to amazon s3?

Answer:
AWS VPC Endpoint for s3
Here's why:
. private connectivity - vpc endpoints establish a private connection between your vpc and the s3 service using aws' internal network. This means your ec2 instance can reach s3 without traversing the public internet.
. Security - by keeping traffic within the aws network, you enhance the security of your data and reduce the risk of unauthorized access.
. Performance - vpc endpoints can improve the performance of s3 access, especially for large volumes of data, by avoiding potential intersnet congestion.

How to setup:
1. Create a vpc endpoint - in the vpc console, create a vpc endpoint of type gateway and select the s3 service.
2. Modify route tables. Update your vpc route tables to direct s3 traffic to the vpc endpoint.
3. Configure s3 bucket policies (optional) - if necessary, adjust your s3 bucket policies to restrict access to the vpc endpoint only.

Important considerations:
. Pricing - vpc endpoints are charged based on the amount of data processed. Be aware of the costs associated with data transfer.
. Endpoint policty (optional) - for even fine-grained control, you can attach an endpoint policy to restrict access to specific s3 buckets or actions.

# 1

a company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500GB. Each site has a high-speed internet connection. The company wants to aggregate the data from all these global sites as quickly as possible in a single amazon s3 bucket. The solution must minimize operational complexity. Which cloud solution meets these requirements?

Answer: AWS Snowball Edge
why aws snowball edge: 
. High-speed data transfer - snowball edge is a physical device that can be shipped to each site. It can store up to 80TB of data and has high-speed network interfaces for rapid data transfer over the site's high-speed internet connection.
. On-device processing: snowball edge has compute capabilities, allowing it to preprocess or aggregate data before transferring it to the cloud. This can reduce the amount of data transferred and speed up the overall process.
. Direct integration with s3: snowball edge is designed to work seamlessly with amazon s3, making it easy to transfer the collected data directly into yoru desired s3 bucket.
. Minimal operational complexity - the process of collecting data using snowball edge is relatively simple. You order the device, configure it, ship it to the site, collect the data, and ship it back to aws. Aws handles the data transfer and ingestion into s3.

Other options:
. Direct upload: while possible with high-speed connections, directly uploading 500GB daily from multiple sites could lead to  bandwidth constraints or network congestion, potentially slowing down the process. 
. Aws datasync: this service is useful for ongoing data transfer, but it might not be the most efficient for the initial bulk transfer of large data volumes from multiple locations.
. Aws transfer family - this service is primarily focused on file transfer protocols and might not be the most suitable for the specific requirements of this scenario.

Key advantages of snowball edge:
. Speed - the physical transfer process is often faster than network transfers for large datasets.
. Reliability - snowball edge is designed for secure and reliable data transport.
. Offline capability - it can collect data even if the site temporarily loses internet connectivity.

Overall, aws snowball edge provides a streamlined, efficient, and reliable solution for aggregating large volumes of data from multiple global locations into a single amazon s3 bucket.

# 2
A company needs the ability to analye log files of its proprietary application. The logs are stored in json format in an amazon s3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture. What should the aws solutions architect do to meet these requirements with the least amount of operational overhead?

Answer:
recomment Amazon athena. Here's why:
1. Serverless architecture - athena is a serverless query service, eliminating the need for infrastructure management or setup. This significantly reduces operational overhead.

2. Direct s3 integration - athena can query data directly in amazon s3 without the need for data extraction, transformation or loading (etl) processess. This simplifies the existing architechure and minimizes changes.

3. Json support - athena has built-in support for querying json-formatted data, making it a perfect fit for the company's log files.

4. On-demand queries - athena is designed for ad-hoc queries, meaning you only pay for the queries you run. This aligns with the requirement for running simple on-demand queries.

5. SQL-like interfaces - athena uses a familiar sql-like interface, making it easy for analysts to write and execute queries.

Here's a simplified plan:
1. Create an athena table - define an athena table that points to the s3 bucket containing the log files. Specify the json structure of the log files in the table definition.
2. Start querying - use the athena console or api to write sql queries to analyze the log data. Athena will automatically parse and query the json files in S3.

Additional considerations:
. Cost - while athena is serverless, you will be charged for the amount of data scanned during queries. Consider using partitioning or compression in s3 to optimize costs.
. Performance - if the log files are extremely large or queries become complex, consider using athena's federated query feature to query data in other data sources or use a more specialized log analytics solution like amazon opensearch service.


# 3
A company uses aws organizations to manage multiple aws accounts for different departments. The management account has an amazon s3 bucket that contains project reports. The company wants to limit access to this s3 bucket to only users of accounts within the organization. What should an aws solutions architect do to meet these requirements?

Answer:
Here's a breakdown of the solution and why it's the best approach:

**Solution:**

1. **Create an IAM Role in the Management Account:**

   * Create a new IAM role specifically for cross-account access to the S3 bucket.
   * Attach a policy to this role that explicitly allows:
      * `s3:ListBucket` on the specific project reports bucket.
      * `s3:GetObject` on the objects within the bucket.
      * (Optional) `s3:PutObject` if you want users from other accounts to be able to upload new reports.

2. **Share the IAM Role with Other Accounts in the Organization:**

   * In the IAM role's trust policy, add a condition that specifies the AWS Organization ID. This ensures that only accounts belonging to your organization can assume the role.

3. **Grant Users in Member Accounts Access to Assume the Role:**

   * In each member account where you want users to access the reports:
      * Create IAM policies for specific users or groups.
      * Grant these policies the permission to `sts:AssumeRole` on the cross-account role you created in the management account.

**Why This Approach is Ideal:**

* **Security:** By using an IAM role and explicitly defining permissions, you strictly control who can access the S3 bucket and what actions they can perform. The Organization ID restriction ensures that only trusted accounts within your company can gain access.
* **Scalability:** This solution works well as your organization grows. You can easily add new accounts to your organization and grant them access to the reports by simply allowing them to assume the shared role.
* **Least Privilege:** The principle of least privilege is followed. Users in member accounts are only granted the necessary permissions to assume the role, not direct access to the S3 bucket. This reduces the risk of accidental or unauthorized actions.

**Additional Considerations:**

* **Bucket Policy:** While the IAM role approach is preferred, you could also add a bucket policy to the S3 bucket itself. The policy would need a condition to check if the requesting principal is a role from a member account within your organization. However, this can be more complex to manage.
* **Logging and Monitoring:** Enable S3 access logging and CloudTrail in the management account to track access to the project reports bucket. This provides an audit trail and helps you identify any unauthorized access attempts.

