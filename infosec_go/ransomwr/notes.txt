A.2 Tasks

Once notified of potential ransomware the following steps should be taken.

Important â€“ Do not shut down/restart any computers/servers during investigation.

Identify affected devices. Take all potentially affected devices offline immediately via network or SentinelOne portal. If many devices are affected it may be faster to isolate by physically turning off a network switch.

Assign manager to notify employees that computer services are unavailable.

Perform prelimary investigation of device(s) in question. Is there a confirmed infection?

If infection is confirmed call SentinelOne breach line for aid - 1-855-868-3733

Identify what backups are available of the data affected and also validate that the backups are usable.

Determine if the incident affects any customers. Contact Attorney regarding notification legal requirements

A.3 Follow up

Determine if passwords need to be reset or if accounts need to be locked out.

Conduct interviews with employees, customers, vendors, or any involved parties to help identify weaknesses and the source of the breach.
A random assortment of security tips from someone who has done a good chunk of IR in my career. This obviously isn't a replacement for calling an IR team. You don't need cyber insurance necessarily but you do need an IR partner.

Prep work: Appropriate for all environments.

MFA on every single possible point possible. SMS MFA is dogshit.

Set up some CanaryTokens as triggers for probable ransomware and/or exploitation ( Canarytokens )

Set up a status page (a million options but I like uptimerobot) to handle your mass communication

Execute some AD hardening (if you are using AD) Ping Castle is a good place to start.

Eliminate all inbound port forwarding using solutions like Azure App Proxy.

Put all your admin accounts into "Protected Users" group and make sure your domain forest is at least Server 2016 (this prevents mimikatz and similar attacks by not caching credentials for admins)

Segment your admins into at least 3 layers. Domain > Server > Workstation. Admins can have multiple accounts. PAM solutions are better but not everyone can afford them.

Use FIDO2 tokens for all your AAD admins (phones can now be used in Windows 11)

Disable WMI and WS-Management. Extremely useful as an admin. A million times more useful to the attacker for horizontal movement. Use an RMM instead.

ENSURE YOUR BACKUP SYSTEM IS NOT JOINED TO THE DOMAIN. It should have unique permissions, intentionally not be SSO and use a "Pull" rather than push method to avoid compromised devices from overwriting good backups.

Offline and Immutable backups are absolutely essential. If you don't have this today, go do this now.

Prep work: Appropriate for higher security environments.

Audit Logs on AD/AAD environment are essential. Reviewing them post breach is important GPO, OU permission changes etc. are common persistence attacks that are very difficult to spot.

Use Privileged Access Workstation model for accessing secure systems that have aggressive logging to a source that is read only.

Deploy a PAM solution that assigns permissions and access tokens on an on demand method.

Deploy AppLocker (or equivalent) to whitelist executable applications on at least your PAW stations.

Consider how you want to handle external connection monitoring (where laws/policies allow). Identifying the C&C control paths and locking them down is important.

Prepare a scenario where you have to restore data only to your entire environment. It is unlikely to be used but you should be prepared for it.

If you are development shop, take special care of how your deployments are certified and protected. The root certs should be offline 100% of the time.

Post breach:

Scope identification is by far the most difficult stage. Being cautious is generally a good idea. If Admin accounts are potentially compromised a full network outage during investigation is a good idea.

Talk to an IR team.

If you can't figure out how they got in, close all the paths in and open them one at a time.

All accounts created in the past several months should be reviewed very closely. A common technique is for them to generate sock puppet accounts with marginally elevated permissions as a path back in.

Have a plan with your executive team that accounts for downtime. That should include vendor facing communication, user facing communications, public facing communications etc. What is the order your environment should come up in if everything is down? (Generally payroll first)

Upon alert, suspend backup retention policies. You don't want backups rolling off.

Get a copy of your cyber insurance policy. Make sure you're meeting the requirements that you probably have no idea were in there. (Yes, I'm bitter).

Conduct interviews with employees, customers, vendors, or any involved parties to help identify weaknesses and the source of the breach.

Be very careful with this. Knowledge of the breach can be seriously damaging to your business reputation. Communication plan needs to be rock solid.

Make sure you identify WHO is communicating up front. This is very, VERY important. You don't want some helpdesk guy taking calls from CNN or local news. The communication plan should be a link to a document, not a task.

If many devices are affected it may be faster to isolate by physically turning off a network switch.

I would go with disable vs. "physically turning off", unless you are a very small shop, have no servers and people around 24x7. Admin down is probably a lot faster than driving in. Don't put yourself in a box.

Identify what backups are available of the data affected and also validate that the backups are usable.

Use some terminology like "last known good", as this time is likely to change through more investigation.

Determine if passwords need to be reset or if accounts need to be locked out.

Set all accounts to require a reset on login (non service). Go scorched earth. Recovering from a breach is pretty freaking hard. Doing it twice sucks to infinity.

get ahold of your cyber insurance policy. There is a good chance there are things in there you are not aware of.. Like network microsegmentation, clean room investigate. Snapshots/backups immediately AFTER the hit, etc. If you get hit with ransomware, the lawyers and insurance company are going to dictate WAY more than you think they should.


Warning: This started as my experience and turned into my crypto story. Read at your own risk. Just know my intention was to give some pointers to anyone who has not been through one.

Ok... I have been through this. Before this was a thing... We were hit Friday afternoon. Didn't find out we were until Monday morning and people were having weird messages on login about profiles (roaming profiles). Then the real slow lump in throat swallow came when one of the accounting people couldn't get to the file share to update for the weekly Monday meeting. This actually wasn't abnormal as sometimes... well... just know that we had stuff that regularly happened that we could never find the cause and the only fix would be to move the files to a new server so we just dealt with it when it came up every month or so. Quick 20second fix, restart a service and boom all good. Anyway, tried to RDC into server... I could but... something was wrong. File on the desktop would confirm. Any file we tried to access was locked etc. etc. etc. you know the drill.

Immediately let my manager know who then took the job of relaying to all the key players what we were thinking we had and how bad it is. This was the play through the entire process. I would primarily run info to my manager that would then relay.

I would love to tell you that in the end we found client 0 but we did not. I still hold to this day that I have a high suspicion that what happened was a Jr. Sysadmin had let a vendor come in through a team viewer session to update a server. Note, we used Bomgar and I should have pushed to get a reverse session into that machine using that but ...vendors! My guess is the session was left open and someone found it and from there, boom.

Thankfully, the thing never left the VLAN it was on which is sadly the server VLAN but that means that we didn't have to reimage ~450 on top of getting the servers up and running. We also discovered that it looks like it moved via mapped drives so from then on out, I only map drives temporarily and just make shortcuts via scripts for things that are needed to be connected to. We know it did that because the DC was ok as well as some other random servers that are services but nobody connects for actual data. everything hit was ones that had mapped drives that the user that logged in had to the server hosting the software that was being updated. Oh and also, the server that they came through did not get ransomed. We found what we believe were traces of the files used to do the job but was not confirmed.

Thankfully that means that the backups were ok however the big things that were hit were the Exchange server, File Server, and one LoB server (which was the easiest to bring up).

I verified that I could get to the backups, launched one (Veeam) to make sure it was not infected before restoring that particular one (was good). Once I had that, we all met, discussed what had happened, what we could do. They were interested in paying the ransom however because it happened on a Friday and not found until Monday it was past the point of being able to do so. They contacted the lawyer to go over all of the legal ramifications. We had cyber insurance however that was looked into because while you have it, it's not just a clear cut thing. Remember, insurance on any level is a game of chicken. At that point in time, because we could not pay the ransom anymore it was decisions that were not mine to make. My job turned quickly into getting us back up as quickly as possible with the least amount of loss.

This is where things went nuts. our Backup of exchange was 12TB. We were using a D2D device to do the backups. We do launch backups and pull mailboxes and files and mail from there all the time so no worries. Went to start the restore... "Estimated time: 3 WEEKS" ?!?!??!?!?!?!?!? Yup. So short story here is this.... UPDATE YOUR FIRMWARE ON YOUR STUFF! Turns out even though our device was phoning home and reporting if there was an issue, it was not reporting and checking in with the software and firmware to see if it had available updates. Not only that but somehow the serial number of our device was also registered to a device in South America. No, I can't make this stuff up... LITERALLY. I'm not going to name the company but if I keep going some may figure it out. After working with them, yes, there was an issue (I was so relieved for them to tell me there was an issue) with the phone home of the device and we were like 2 major revisions behind. Apparently we were able to take a process that takes two weeks or so during a normal update process due to all the logs you have to submit back and forth over and over again before you can actually schedule the update with them, which they have to be the ones to do the update, not you. We were able to take this two week process and get it done in a few hours. THEN... something they RARELY ever do, they did a second update on the system on the same day. They made us wait 8 hours before they would do the second. Something about wanting it to run to see if any error logs were generated before they did the next.

Turns out that there was a firmware bug in the version we originally had where the rehydration process was super messed up. The dedupe process worked fine and writing files was fine but the rehydration just was not good at all. I guess the operation is different if you are launching the vm inside of Veeam because id didn't take that long (in my opinion) to launch or to recover a mailbox).

Still... took us from 3 weeks down to 2 days to restore. It literally did take that long. After that we did the file server (email was more important) which took about 9 hours or so. Some of the other servers that went down were rebuilt as they were for applications. Yes we had backups but the time it was taking to restore, and the fact that we didn't have anything else to do in that time while waiting... was easier to restore. It was actually harder to locate the licenses and whatnot that we needed to reinstall the software.

In the end, after much deliberation and what we know that we could get back up in ~4 days total, they did go and activate the insurance. One of the issues with insurance companies, at least back then is that they had to send their guys out to help investigate. Well that was a dud. They never found out where it came from, never looked at really any logs or did anything really. ...except for gather information so they could figure out what services they could sell us. They were supposed to go through every desktop and make sure it was clean using their "special scanning tool". Remember how I said we use Bomgar. Yea, we even helped them and created an account so they could connect to every PC. Maybe they didn't realize that Bomgar records your sessions and has good logging. They did MAYBE 20-30 machines and actually scanning machines... maybe 10-15. It honestly seemed like they were only there to try to find out if we were doing things incorrectly. We weren't perfect but user accounts and such were fine. MFA wasn't a thing back then. Like I said really the only thing that happened was a left open team viewer session.

Anyway they activated that, got $$. Nothing data-wise was lost save some trivial stuff and stuff that could be rebuilt as it happened on a Friday.

Fun part is that we actually brought in the Secret Service. I never knew that this is the stuff they actually do. I was brought down to have a meeting with our GM/VP/2nd in command, Secret Service Agent, and My manager at the time (IT Manager). Short version is that I was asked to give a full sitrep (yea when talking to the secret service I am going to call it that lol), what I believe happened, what we have done, what we are doing etc. In the end he turned and looked at the GM and said "you are doing everything you are supposed to be doing by the book" which was one of the coolest pats on the back I've gotten. We then put together a training that he came and trained our entire staff on. He was very impressed with what we were able to not only put together but our facility for training, the way it was setup as well as the edits that I made to his boiler plate presentation to fit our environment. Cool dude, I got to talk to him and work with him quite a lot over the few days he was there. So FYI, if you want to do a security training, call the secret service and apparently they will come and do it for you for free (TAX $$ actually being used for good). That was a long while ago so it may not anymore.

I sort of like a flowchart, starting with how was ransomware found:

Did the backup program throw an alert and say that a lot of files were changed and it tripped off the heuristic alerts?

Did machines start doing a lot of grabbing files that the user normally never accessed?

Did a user try to remote into machines they never use?

Is there a lot of disk activity?

Worst of all, are notices popping up everywhere?

The way you found out about it can be a degree of how fscked you are. Worst case, and nobody has any experience, call a MSSP. If it was just files changed, and you can trace Patient Zero, you can restore or roll back and then isolate and deal with the offending machine.

Ransomware is about prevention. There are five things I do just to keep ransomware from reaching all corners of the organization:

First, PAWs. AD or global admin is done through dedicated machines that never are used for any daily driver stuff. This ensures that a compromised machine doesn't compromise AD. For machine admin tasks, domain users granted admin via a GPO, are good.

Second, a separate directory server running LDAP/FreeIPA/IdM which runs the network appliances, consoles, and backup stuff. FreeIPA/IdM allows you to use 2FA "baked into" the LDAP login which gives effective 2FA protection to iDRAC consoles, and other items that may not have 2FA available.

Third, I set up MinIO, completely separate from everything else. It needs a load balancer, and ideally 4+ nodes, but you can use one node and ZFS. If the box is locked down well, it provides S3 write lock protection to buckets stored on it, so you get tamper resistance that way. If the attackers can't get root access to the MinIO boxes, the worst they can do is write garbage to the buckets in order to fill up the drive to prevent new backups from succeeding, and to cause issues with restores.

Fourth, tapes were used as a secondary backup source. When media is offline, it can't be reached unless someone physically does it.

Fifth, for the critical documents, they were backed up in a second backup, and were saved to WORM tape media, which, when the tape's contents expired, the tape would be shredded. This way, the data would be protected, barring physical destruction, or compromised tape drive firmware.

After getting core things in place, it is then about defense in depth, especially endpoints.

Nist cybersecurity framework
https://www.nist.gov/cyberframework
